# EP321: Be Defensive when Iterating over Arguments

**Book Reference**: "Effective Python" (3rd Edition), Item 21, Chapter 3: Functions  
**Tier**: 1 (High Impact)  
**Status**: Tier 1 Priority

## Overview

Be defensive when iterating over arguments that could be iterators. Functions that iterate over the same argument multiple times will fail silently if passed an iterator (which gets exhausted) instead of a container. Always check if the argument is an iterator or convert it to a list.

## Problem

Functions that iterate multiple times fail with iterators:

```python
# ❌ Function assumes argument is a container, not iterator
def normalize(numbers):
    total = sum(numbers)        # First iteration - consumes iterator
    result = []
    for value in numbers:       # Second iteration - EMPTY if iterator!
        percent = 100 * value / total
        result.append(percent)
    return result

# This works fine with lists:
data = [15, 35, 80]
result = normalize(data)  # [15.0, 35.0, 80.0] - as expected

# But fails silently with iterators:
def read_visits(path):
    with open(path) as f:
        for line in f:
            yield int(line)

visits = read_visits('data.txt')
result = normalize(visits)  # [15.0, 35.0, 80.0] first time
result = normalize(visits)  # [] - EMPTY! Iterator exhausted

# ❌ More subtle example
def analyze_data(items):
    count = len(items)          # This might work...
    total = sum(items)          # First iteration
    average = total / count
    
    filtered = [x for x in items if x > average]  # Second iteration - FAILS
    return filtered

# Works with list: analyze_data([1, 2, 3, 4, 5])
# Fails with iterator: analyze_data(x for x in [1, 2, 3, 4, 5])
```

## Solution

Convert iterators to containers or detect them explicitly:

```python
# ✅ Solution 1: Convert to list (simple but uses memory)
def normalize(numbers):
    numbers = list(numbers)     # Defensive conversion
    total = sum(numbers)        # Now safe to iterate multiple times
    result = []
    for value in numbers:
        percent = 100 * value / total
        result.append(percent)
    return result

# ✅ Solution 2: Single iteration (most efficient)
def normalize(numbers):
    numbers_list = list(numbers)  # Convert once
    total = sum(numbers_list)
    return [100 * value / total for value in numbers_list]

# ✅ Solution 3: Detect iterators explicitly
import collections.abc

def normalize(numbers):
    if isinstance(numbers, collections.abc.Iterator):
        numbers = list(numbers)
    
    total = sum(numbers)
    return [100 * value / total for value in numbers]

# ✅ Solution 4: Raise clear error for iterators
def normalize(numbers):
    if isinstance(numbers, collections.abc.Iterator):
        raise TypeError("Cannot normalize iterator, use list() first")
    
    total = sum(numbers)
    return [100 * value / total for value in numbers]

# ✅ Solution 5: Use itertools.tee for complex cases
import itertools

def analyze_data_stream(items):
    # Split iterator into two independent iterators
    items1, items2 = itertools.tee(items, 2)
    
    total = sum(items1)         # Use first iterator
    count = sum(1 for _ in items2)  # Use second iterator
    
    # For more iterations, need more tee() calls or convert to list
    items_list = list(items) if isinstance(items, collections.abc.Iterator) else items
    average = total / count
    return [x for x in items_list if x > average]
```

## Detection Criteria

This rule detects:

1. **Multiple iterations over same parameter**:
   ```python
   def func(items):
       total = sum(items)      # First iteration
       for item in items:      # Second iteration - triggers EP321
           process(item)
   ```

2. **Built-in functions + explicit loops**:
   ```python
   def func(data):
       count = len(data)       # Iteration (if iterator, this fails anyway)
       maximum = max(data)     # Another iteration - triggers EP321
       for item in data:       # Third iteration
           process(item)
   ```

3. **Multiple comprehensions over same parameter**:
   ```python
   def func(items):
       filtered = [x for x in items if x > 0]    # First iteration
       mapped = [x*2 for x in items]             # Second iteration - triggers EP321
   ```

## When Not to Apply

- **Single iteration functions**:
  ```python
  # ✅ Only iterates once - safe for iterators
  def process_all(items):
      return [process(item) for item in items]
  ```

- **Parameters documented as containers**:
  ```python
  # ✅ Documented expectation
  def analyze_list(data: list[int]) -> float:
      """Analyze a list of integers. Must be a list, not iterator."""
      return sum(data) / len(data)
  ```

- **Private/internal functions with known usage**:
  ```python
  # ✅ Internal function with controlled usage
  def _helper(known_list):
      # We know this is always called with lists
      return sum(known_list) / len(known_list)
  ```

## Book Context

From Item 21: "When a function takes an iterator as an argument, you need to be careful about how many times you iterate over it. Iterators only produce their results a single time."

Key insights:
- Iterators are consumed after first iteration
- Many built-in functions create iterators (map, filter, etc.)
- Generator functions return iterators
- File objects are iterators
- The bug is silent - no error, just wrong results

## Gap Analysis

**Why no existing tool detects this**:
- Requires function-level analysis of parameter usage
- Need to track multiple iteration patterns
- Complex to distinguish safe vs unsafe iteration
- Static analysis can't always determine if parameter is iterator

## Implementation Notes

**AST Pattern Detection**:
1. Analyze function parameters in function definitions
2. Track usage of each parameter in function body
3. Count iteration operations (for loops, comprehensions, built-ins)
4. Flag parameters used in multiple iterations

**Iteration Detection**:
- For loops: `for x in param`
- Comprehensions: `[x for x in param]`
- Built-in functions: `sum(param)`, `max(param)`, `len(param)`
- Method calls: `param.join()`, etc.

**Complexity Considerations**:
- Nested function calls passing parameters
- Partial iterations (break/continue)
- Conditional iterations

## Examples from the Wild

```python
# ❌ Data processing pipeline bug
def calculate_statistics(values):
    total = sum(values)         # First iteration
    count = sum(1 for _ in values)  # Second iteration - EMPTY!
    mean = total / count        # Division by zero!
    
    variance = sum((x - mean)**2 for x in values) / count  # Third iteration - EMPTY!
    return {'mean': mean, 'variance': variance}

# ❌ File processing bug
def analyze_log_file(lines):
    error_count = sum(1 for line in lines if 'ERROR' in line)  # First iteration
    warning_count = sum(1 for line in lines if 'WARN' in line)  # Second - EMPTY!
    
    return {
        'errors': error_count,
        'warnings': warning_count,  # Always 0 if lines is iterator!
    }

# ❌ Batch processing bug
def process_batch(items):
    if not any(item.is_valid() for item in items):  # First iteration
        return None
    
    # Second iteration - EMPTY if items is iterator!
    return [process(item) for item in items if item.is_valid()]

# ✅ Fixed versions
def calculate_statistics(values):
    values = list(values)  # Defensive conversion
    total = sum(values)
    count = len(values)
    mean = total / count
    variance = sum((x - mean)**2 for x in values) / count
    return {'mean': mean, 'variance': variance}

def analyze_log_file(lines):
    lines = list(lines)  # Convert to list
    error_count = sum(1 for line in lines if 'ERROR' in line)
    warning_count = sum(1 for line in lines if 'WARN' in line)
    return {'errors': error_count, 'warnings': warning_count}

def process_batch(items):
    items = list(items)  # Ensure we can iterate multiple times
    if not any(item.is_valid() for item in items):
        return None
    return [process(item) for item in items if item.is_valid()]
```

## Advanced Patterns

```python
# ✅ Memory-efficient single pass when possible
def calculate_stats_single_pass(values):
    total = 0
    count = 0
    squares = 0
    
    for value in values:  # Single iteration
        total += value
        count += 1
        squares += value * value
    
    if count == 0:
        return None
        
    mean = total / count
    variance = (squares - total * total / count) / count
    return {'mean': mean, 'variance': variance}

# ✅ Using itertools.tee for multiple independent iterations
import itertools

def analyze_with_tee(items):
    # Create three independent iterators
    iter1, iter2, iter3 = itertools.tee(items, 3)
    
    positive_count = sum(1 for x in iter1 if x > 0)
    negative_count = sum(1 for x in iter2 if x < 0)
    total = sum(iter3)
    
    return {
        'positive': positive_count,
        'negative': negative_count,
        'total': total
    }

# ✅ Explicit iterator detection and handling
import collections.abc

def smart_normalize(numbers):
    # Check if we have an iterator
    if isinstance(numbers, collections.abc.Iterator):
        # Convert to list with warning
        import warnings
        warnings.warn("Converting iterator to list for multiple iterations", 
                     UserWarning)
        numbers = list(numbers)
    
    total = sum(numbers)
    return [100 * value / total for value in numbers]
```

## Common Iterator Sources

```python
# These all return iterators that get exhausted:
map_result = map(str.upper, strings)        # map object
filter_result = filter(None, items)         # filter object  
generator = (x*2 for x in range(10))       # generator expression
file_lines = open('file.txt')              # file object
dict_keys = my_dict.keys()                  # dict_keys view (Python 3)

# Generator functions return iterators:
def fibonacci():
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a + b

fib = fibonacci()  # This is an iterator!
```

## Related Rules

- **EP318**: Parallel Iteration with zip() (Tier 1) - safe iteration patterns
- **EP320**: Loop Variables After Loop Ends (Tier 1) - related safety concerns

## Error Message Format

```
EP321 Function iterates over parameter multiple times, be defensive with iterators
→ 'Effective Python' (3rd Edition), Item 21, Chapter 3: Functions
→ Bug Prevention: Iterators get exhausted, causing silent failures on second iteration
→ Example: def f(x): sum(x); for i in x: ... → def f(x): x=list(x); sum(x); for i in x: ...
```

## Performance Considerations

- `list()` conversion uses memory but ensures correctness
- Single-pass algorithms are most efficient when possible
- `itertools.tee()` has memory overhead for independent iterators
- Consider generator functions for large datasets that don't need multiple iterations